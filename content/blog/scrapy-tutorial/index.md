---
tags: 
- Soft
comments: true
date: 2015-09-06T15:16:53Z
draft: true
title: 'Scrapy: краткое руководство'
---

Почти у каждого программиста, рано или поздно, возникает простая задача: вытащить данные с сайта. Для нее можно использовать разные решения: парсить страницы регэкспами, использовать XPath или [CSS-селекторы](http://www.w3schools.com/cssref/css_selectors.asp) - но тогда большую часть кода для обработки ошибок, логгирования, постобработки придется переизобретать или использовать библиотеки ЯП, вроде [lxml](http://lxml.de/) для Python. Однако, самый удобный для повседневной жизни вариант - использовать all-in-one фреймворк для всех задач, связанных с парсингом и обходом страниц сайта. Для Python такой фреймворк существует, и называется [Scrapy](http://scrapy.org/).

<!--more-->

### Установка и начало

Установить Scrapy достаточно просто: если у вас уже стоит [PIP](https://pip.pypa.io/en/stable/), достаточно набрать в консоли:

>pip install scrapy

Также, можно воспользоваться встроенной библиотекой setuptools:

>easy_install scrapy

Для Linux потребуется также поставить девелоперские пакеты:

>sudo apt-get install python-dev 

Обратите внимание, что scrapy **несовместим с Python 3**. Если у вас стоит Python3 в системе по умолчанию - убедитесь, что фреймворк установлен именно во 2 версию.

Scrapy - фреймворк, и работает с проектами. Чтобы создать новый проект - наберите в консоли:

> scrapy startproject HelloWorld

где HelloWorld - название проекта. В папке, соответствующей названию проекта, будет автоматически создан каркас проекта - он состоит из следующих файлов:

>├── HelloWorld

>│   ├── \_\_init\_\_.py

>│   ├── items.py

>│   ├── pipelines.py

>│   ├── settings.py

>│   └── spiders

>│       └── \_\_init\_\_.py

>└── scrapy.cfg

Каждый проект состоит из нескольких частей: это **items** - описание моделей объектов, которые собирает наш ноутбук; **spiders** - пауки, собирающие информацию из сети и записывающие ее в модели; **pipelines** - пост-обработчики, которые преобразуют собранную информацию в нужный нам формат (например, записывают в БД или в файл CSV). В проекте может быть несколько пауков, равно как и моделей данных. Файл **scrapy.cfg** описывает глобальные настройки проекта, когда как **settings.py** - общие настройки для пауков (например, используемый User-Agent).

### HelloWorld

Рассмотрим небольшой проект, который собирает данные о дрессировщиках из базы APDT - американской ассоциации дрессировщиков. Интересующий нас поиск расположен здесь: [https://apdt.com/trainer-search/](https://apdt.com/trainer-search/)
